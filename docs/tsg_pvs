 
This section captures steps to troubleshoot and resolve some of the errors faced while using OpenEBS PVs. The steps described and commands used in this document are mostly generic and are applicable on any common linux platform/kubernetes environment. Contributions are welcome to add steps for scenarios/environments not covered !!

In cases where the steps provided are not helpful in rectifying the problem, please raise an issue and/or join the #openebs-users slack channel. We shall be happy to help !!

Application pod is stuck in ContainerCreating state upon deployment
Obtain the output of the kubectl describe pod <application_pod> and check the events

If the error message executable not found in $PATH is found, check whether the iSCSI initiator utils are installed on the node/kubelet container (rancherOS, coreOS). If not, install the same and retry deployment.

If the warning message FailedMount: Unable to mount volumes for pod <>: timeout expired waiting for volumes to attach/mount is seen to persist:

Check whether the PVC/PV are created successfully and the OpenEBS controller and replica pods are running. These can be verfied using the kubectl get pvc,pv and kubectl get pods command.

If the OpenEBS volume pods are not created, and the PVC is in pending state, check whether the storageclass referenced by the application PVC is available/installed. This can be confirmed using the kubectl get sc command. If this storageclass is not created, or is improperly created without the appropriate attributes, recreate the same and re-deploy the application.

Note: Ensure that the older PVC objects are deleted before re-deployment.

If the PV is created (In bound state), but replicas are not running/are in pending state, perform a kubectl describe <replica_pod> and check the events. If the events indicate FailedScheduling due to Insufficient cpu, NodeUnschedulable or MatchInterPodAffinity and PodToleratesNodeTaints, check the following:

Whether there are replica count is equal to or lesser than available, schedulable nodes
Whether there are enough resources on the nodes to run the replica pods
Whether the nodes are tainted & if so, whether they are tolerated by the OpeneEBS replica pods
Ensure that the above conditions are met and the replica rollout is successful. This will ensure application enters running state.

If the PV is created and OpenEBS pods are running, use the iscsiadm -m session command on the node (where the pod is scheduled) to identify whether the OpenEBS iSCSI volume has been attached/logged-into. If not, verify network connectivity between the nodes.

If the session is present, identify the SCSI device associated with the session using the command iscsiadm -m session -P 3. Once it is confirmed that the iSCSI device is available (check the output of fdisk -l for the mapped SCSI device), check the kubelet & system logs including the iscsid & kernel (syslog) for information on the state of this iSCSI device. If inconsistencies are observed, execute the filesyscheck on the device fsck -y /dev/sd<>. This should esult in volume being mounted to the node.

In OpenShift deployments, this error is encountered with the OpenEBS replica pods continuously restarting, i.e., are in crashLoopBackOff state. This is due to the default "restricted" security context settings. Edit these settings via oc edit scc restricted to get the application pod running:

allowHostDirVolumePlugin: true
runAsUser: runAsAny
Application pod enters CrashLoopBackOff state
This is a result of failed operations by the application in the container. Typically this is caused due to failed writes on the mounted PV. To confirm this, check the status of the PV mount inside the application pod.

Perform a kubectl exec -it <app> bash (or any available shell) on the application pod and attempt writes on the volume mount. The volume mount can be obtained either from the application specification ("volumeMounts" in container spec) OR by performing a df -h command in the controller shell (the OpenEBS iSCSI device will be mapped to the volume mount).

The writes can be a attempted using a simple command like echo abc > t.out on the mount. If the writes fail with Read-only file system errors, it means the iSCSI connections to the OpenEBS volumes were/or-continue-to-be lost. This can be confirmed by checking the node's system logs including iscsid, kernel (syslog) and the kubectl logs (journalctl -xe, kubelet.log).

Failed iSCSI connections are often a result of

flaky networks (can be confirmed by ping RTTs, packet loss etc.,) or failed networks between:

OpenEBS PV controller and replica pods
Application and controller pods
Node failures

OpenEBS volume replica crashes/restarts due to software bugs

In all the cases above, loss of the device for a period greater than the node iSCSI initiator timeout causes the volumes to be remounted as RO.

In certain cases, the node/replica loss can lead to the replica quorum not being met (i.e., less than 51% of replicas available) for an extended period of time, causing the OpenEBS volume to be presented as a RO device.

The steps to ensure application recovery in the above cases are:

Resolve the system issues which caused the iSCSI disruption/RO device condition. Depending on the cause, the resolution steps may include recovering the failed nodes, ensuring replicas are brought back on the same nodes as earlier, fixing the network problems etc.,

Ensure that the OpenEBS volume controller and replica pods are running successfully with all replicas in RW mode. This can be confirmed using the command curl GET http://<ctrl ip>:9501/v1/replicas | grep createTypes

If any one of the replicas are still in RO mode, wait for the sync to complete. If all the replicas are in RO mode (this may occur when all replicas re-register into the controller within short intervals), the OpenEBS volume controller has to be restarted. This can be performed using the command kubectl delete pod <pvc-ctrl>. Since it is a Kubernetes deployment, the controller pod is restarted successfully. Once done, verify that all replicas transition into RW mode.

Unmount the stale iscsi device mounts on the application node. Typically, these devices are mounted here:

/var/lib/kubelet/plugins/kubernetes.io/iscsi/iface-default/<target-portal:iqn>-lun-0

Identify whether the iSCSI session has been re-established post the failure. This can be verified using iscsiadm -m session, with the device mapping established using iscsiadm -m session -P 3 and fdisk -l.

Note: Sometimes, it has been observed that there are stale device nodes (scsi device names) present on the Kubernetes node. Unless the logs confirm that a re-login has occured once the system issues were resolved, it is advisable to perform the following step after doing a purge/logout of the existing session using iscsiadm -m node -T <iqn> -u

If the device has not already re-logged in, ensure that the network issues/failed nodes/failed replicas are resolved and device is discovered and session re-established. This can be achieved using the commands iscsiadm -m discovery -t st -p <ctrl svc IP>:3260 and iscsiadm -m node -T <iqn> -l respectively

Identify the new SCSI device name corresponding to the iSCSI session (the device name may or may not be the same as before)

Re-mount the new disk into the mountpoint mentioned earlier using the command: mount -o rw,relatime,data=ordered /dev/sd<> <mountpoint>. If the remount fails on account of inconsistencies on the device (unclean filesystem), perform a filesyscheck fsck -y /dev/sd<>

Ensure that the application uses the new mount by forcing its restart on the same node. This can be done via a docker stop <id> of the application container on the node. Kubernetes will automatically restart the pod to ensure the "desirable" state.

While this step may not be necessary most times (as the application is already undergoing periodic restarts as part of the CrashLoop cycle), it can be performed if the application pod's next restart is scheduled with an exponential back-off delay.

Notes: In environments where the kubelet runs in a container (RancherOS, CoreOS, Containerized OpenShift deployment), the iSCSI re-login & remount steps are not needed to be performed explicitly. Instead, it is replaced by the restart of the kubelet container on the node.

Stale data seen post application pod reschedule on other nodes
Sometimes, stale application data is seen on the OpenEBS volume mounts post application pod reschedule. Typically, these applications are kubernetes deployments, with the reschedule to other nodes occurring due to rolling updates.

This occurs due to the iSCSI volume mounts and sessions staying alive/persisting on the nodes even after pod terminate. This behaviour has been observed on some verisons of GKE clusters (1.7.x).

Ideally, the kubelet (iSCSI volume plugin) should tear down mounts and iscsi sessions once the application has been deleted on the node. Failure to do so can result in data being read off the node's page (mount) cache whenever the application is re-scheduled onto it, despite the volume being updated while on a different node.

The workaround for this problem is to perform the following steps:

Unmount the device and logout from existing iSCSI session on stale (non-owning) node
Re-login & remount the volume on the current/scheduled (owning) node
Ensure application pod uses the new mount by restarting it via docker stop
Application and OpenEBS pods terminate/restart under heavy I/O load
This is caused due to lack of resources on the Kubernetes nodes, which causes the pods to be "evicted" under loaded conditions due to the node becoming unresponsive. The pods transition from Running state to unknown state followed by Terminating & before being restarted again.

The above cause can be confirmed from the kubectl describe pod which reveals the termination reason as NodeControllerEviction. More info can be obtained from the kube-controller-manager.log on the Kubernetes master.

This can be resolved by upgrading the Kubernetes cluster infrastructure resources (Memory, CPU)
