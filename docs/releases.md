---
id: releases
title: OpenEBS Releases
sidebar_label: Releases
---

------

<br>





| Release Version                  | Notes                                                        | Highlights                                                   |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| <br/><font size="5">1.3.0</font> | <br/> <font size="4">Latest Release</font><br/> (Recommended)<br/> <br/>[Release Notes](https://github.com/openebs/openebs/releases/tag/1.3.0)<br/> [Upgrade Steps](/docs/next/upgrade.html)<br/> | - Add support to scaleup replicas, replica movement across  pools and replica replacement scenarios. This feature is in alpha state. This feature will work for cStor volumes which are created with existing SPC configuration.<br>- Availability of NDM on different platforms like amd64 and arm64. NDM can now also be compiled in ARM architecture using manual steps.<br>- Added support for provisioning CSI based volume using lease leader election API.<br>- Support of running OpenEBS in Kubernetes 1.16 version. The k8s v1.16 release will stop serving the deprecated API versions in favour of newer and more stable API versions.<br>- Support the addition of resource limit to cStor pool pod using with CSPC configuration. `resource` field on CSPC is used to pass resource limit and requests to `cstor-pool` container and `auxResource` field on CSPC is used to pass `resource limit` and `requests` to other 2 containers such as `cstor-mgmt` and `m-exporter`.<br>- Enhanced backup capability of openebs-velero plugin by checking the status of `Healthy` cStor Volume Replica. In the previous version, a check was performed for healthy CVR during setup only. There might be some chances of cStor pod restart and CVR becomes degraded when we trigger the backup.<br/>- Enhanced CVC(cStor Volume Claim) CR by adding provisioning failure events while provisioning cStor volume using CSI provisioner. <br>- Fixed a bug where cStor volume becoming read-only due to restart of  cstor-volume-mgmt container alone in the target pod.<br>- Fixed wrong status on CVR from Duplicate to Online. Duplicate state on CVR was blocking reconcile on the volume creation in ephemeral case.<br>- Fixed a bug in cStor storage pool YAML. The livenessProbe command  `zfs set io.openebs:livenesstimestamp`  sets the value of io.openebs:livenesstimestamp to the current timestamp. In previous versions, this value was not set properly because of some shell quoting  issues in the command.<br>- Fixed a bug where Jiva volume running on CentOS 7 / RHEL in OpenShift cluster is going to read-only when the controller pod is restarted either due to node restart or upgrades or any other reason. This is due to iSCSI default timeout is replaced with 5 sec if multipath support is enabled on the node. |
| <br/>1.2.0                       | <br/>[Release Notes](https://github.com/openebs/openebs/releases/tag/1.2.0)<br/> [Upgrade Steps](/docs/next/upgrade.html)<br/> | - CSI Driver for cStor Volumes (currently in Alpha) has added support for resizing and volume expansion feature.<br>- The new version of cStor Schema has been introduced to address the user feedback in terms of ease of use for cStor provisioning as well as to make a way in the schema to perform Day 2 Operations using GitOps.<br>- Enhanced error logging of cStor storage pool with a new format for automatic alert generation.<br>- Enhanced Jiva internal snapshot deletion when a number of internal snapshots are more than 10. The deletion happens automatically one by one.<br>- Enhanced velero-plugin to support backup/restore for OpenEBS installed in a different namespace other than `openebs` namespace.<br>- Enhanced NDM to include NodeAttributes in BD and BDC. This will support storing of node name along with the hostname on the BD and BDC CRs.<br>- Fixes BlockDevice CRD by adding node name to the printer column. This feature will get the name of the node to which the BD is attached while performing `kubectl get bd -n <openebs_installed_namespace>`.<br>- Fixes a bug in Jiva when patching and clean up operation of Jiva deployments are failing on Nodes where `hostname` is not the same as `nodename`. The fix will set nodeSelector in deployment and clean-up job after converting the nodename into hostname.<br>- Support of provisioning Local PV in clusters where `nodename` and `hostname` are different.<br>- Support customization of default hostpath for Jiva and Local PV. With the current implementation, customization will not persisted when a restart happened on the Node where maya-apiserver pod is running or when maya-apiserver pod is restarted.<br>- Fixes a bug in NDM where all devices on a node were getting excluded when os-disk-exclude-filter is failed to find the device where OS is installed.<br>- Fixes a bug in snapshot controller where snapshot operation is not throwing any error for invalid `cas-type`. This fix will add `cas-type` validation before triggering the snapshot operations. The valid `cas-type` are cStor and Jiva.<br>- Fixes the bug where more than required BlockDevicesClaims are created for requested SPC in auto pool method.<br>- Fixes an issue in maya-api installer to skip re-apply of default SPC and SC resources if they were installed previously by older version(s) of maya or prior to mayaapi-server restart(s) <br>- Fixes a bug in cStor pool when cStor Storage Pool management creates pool if pool import failed when a disk is not accessible momentarily just at the time of import. cStor storage pool will be in the pending state when this scenario occurs. This PR will fix cStor pool creation by looking on `Status.Phase` as `Init` or `PoolCreationFailed` to create the pool. If `Status.Phase` is any other string, cStor Storage Pool management will try to import the pool. This can cause impact to the current workflow of Ephemeral disks, which works as of now, as NDM can't detect it as different disk and recognizes as the previous disk.<br>- Fixes a bug during a cleanup operation performed on BlockDevice and clean up job is not getting canceled when the state of BlockDevice is changed from `Active` to other states. <br>- Fixes a bug in NDM where cleanup jobs remain in pending state in Openshift cluster. The fix will add service account to cleanup jobs, so that clean-up job pods acquire privileged access to perform the action. |
| <br/>1.1.0                       | <br>[Release Notes](https://github.com/openebs/openebs/releases/tag/1.1.0)<br/> [Upgrade Steps](/docs/next/upgrade.html)<br/> | - Support for an alpha version of CSI driver with limited functionality for provisioning and de-provisioning of cStor volumes.<br/>- Support for the upgrade of OpenEBS storage pools and volumes through Kubernetes Job. As a user, you no longer have to download scripts to upgrade from 1.0 to 1.1, like in earlier releases.<br/>- Enhanced Prometheus metrics exported by Jiva for identifying whether an iSCSI Initiator is connected to Jiva target.<br/>- Enhanced NDM operator capabilities for handling NDM CRDs installation and upgrade. Earlier this process was handled through maya-apiserver. <br/>- Enhanced velero-plugin to take backup based on the `openebs.io/cas-type:cstor` and it will skip backup for unsupported volumes(or storage providers).<br/>- Enhanced velero-plugin to allow users to specify a `backupPathPrefix` for storing the volume snapshots in a custom location. This allows users to save/backup configuration and volume snapshot data under the same location.<br/>- Added an ENV flag which can be used to disable default config creation. The default storage configuration can be modified after installation, but it is going to be overwritten by the OpenEBS API Server.The recommended approach for customizing is to create their own storage configuration using the default options as examples/guidance.<br>- Fixes an issue where rebuilding cStor volume replica failed if the cStor volume capacity was changed after the initial provisioning of the cStor volume.<br/>- Fixes an issue with cStor snapshot taken during transition of replica's rebuild status.<br>- Fixes an issue where application file system was breaking due to the deletion of Jiva auto-generated snapshots.<br>- Fixes an issue where NDM pod was getting restarted while probing for details from the devices that had write cache supported.<br/>- Fixes an issue in NDM where Seachest probe was holding open file descriptors to LVM devices and LVM devices were unable to detach from the Node due to NDM hold on device.<br/>- Fixes a bug where backup was failing where `openebs operator` was installed through helm. `velero-plugin` was checking `maya-apiserver` name and it was different when you have installed via helm based method. Updated velero-plugin to check label of maya-apiserver service name.<br> |
| <br/>1.0.0                       | <br/> [Release Notes](https://github.com/openebs/openebs/releases/tag/1.0.0)<br> [Release Blog](https://blog.openebs.io/openebs-announces-the-availability-of-version-1-0-2bc84e69f27b)<br> [Upgrade Steps](/docs/next/upgrade.html)<br> | - Introduced a cluster level component called NDM operator to manages the access to block devices, selecting & binding BD to BDC, cleaning up the data from the released BD. <br>- Support for using Block Devices for OpenEBS Local PV. <br/>- Enhanced cStor Data Engine to allow interoperability of cStor Replicas across different versions. <br/>- Enhanced the cStor Data Engine containers to contain troubleshooting utilities.<br/>- Enhanced the metrics exported by cStor Pools to include details of the provisioning errors.<br/>- Fixes an issue where cStor replica snapshots created for the rebuild were not deleted, causing space reclamation to fail.<br/>- Fixes an issue where cStor volume used space was showing a very low value than actually used.<br/>- Fixes an issue where Jiva replicas failed to register with its target if there was an error during initial registration.<br/>- Fixes an issue where NDM would create a partitioned OS device as a block device.<br/>- Fixes an issue where Jiva replica data was not clean up if the PVC and its namespace were deleted prior to scrub job completion.<br/>- Fixes an issue where Velero Backup/Restore was not working with hostpath Local PVs.<br/>- Upgraded the base ubuntu images for the containers to fix the security vulnerabilities reported in Ubuntu Xenial.<br/>- Custom resource (Disk) used in earlier releases has been changed to Block Device. |
| <br/>0.9.0                       | <br/>[Release Notes](https://github.com/openebs/openebs/releases/tag/0.9) | Enhanced the cStor Data Engine containers to contain troubleshooting utilities.-Enhanced cStor Data Engine to allow interoperability of cStor Replicas across different versions. -Support for using Block Devices for OpenEBS Local PV.<br>- Support for Dynamic Provisioning of Local PV <br>- Enhanced the cStor Volumes to support Backup/Restore to S3 compatible storage using the incremental snapshots supported by cStor Volumes.<br>- Enhanced the cStor Volume Replica to support an anti-affinity feature that works across PVs.<br>- Enhanced the cStor Volume to support scheduling the cStor Volume Targets along side the application pods that interacts with the cStor Volume.<br>- Enhances the Jiva Volume provisioning to provide an option called DeployInOpenEBSNamespace.<br>- Enhanced the cStor Volume Provisioning to be customized for varying workload or platform type during the volume provisioning.<br>- Enhanced the cStor Pools to export usage statistics as prometheus metrics.<br>- Enhanced the Jiva Volume replica rebuild process by eliminating the need to do a rebuild if the Replica already has all the required data to serve the IO.<br>- Enhanced the Jiva Volume - replica provisioning to pin the Replicaâ€™s to the nodes where they are initially scheduled using Kubernetes nodeAffinity.<br>- Fixes an issue where NDM pods failed to start on nodes with selinux=on.<br>- Fixes an issue where cStor Volume with single replicas were shown to be in Degraded, rebuilding state.<br>- Fixes an issue where user was able to delete a PVC, even if there were clones created from it, resulting in data loss for the cloned volumes.<br>- Fixes an issue where user was able to delete a PVC, even if there were clones created from it, resulting in data loss for the cloned volumes.<br>- Fixes an issue where cStor Volumes failed to provision if the `/var/openebs/` directory was not editable by cStor pods like in the case of SuSE Platforms.<br>- Fixes an issue where Jiva Volume - Target can mark a replica as offline if the replica takes longer than 30s to complete the sync/unmap IO.<br>- Fixes an issue with Jiva volume - space reclaim thread, that was erroring out with an exception if the replica is disconnected from the target. |
| <br/>0.8.2                       | [Release Notes](https://github.com/openebs/openebs/releases/tag/0.8.2) | Enhanced the metrics exported by cStor Pools to include details of the provisioning errors.-Enhanced the cStor Data Engine containers to contain troubleshooting utilities.-Enhanced cStor Data Engine to allow interoperability of cStor Replicas across different versions. <br>- Fixed an issue causing cStor Volume Replica CRs to be stuck, when the OpenEBS<br>   namespace was being deleted.<br>- Fixed an issue where a newly added cStor Volume Replica may not be successfully<br>   registered with the cStor target, if the cStor tries to connect to Replica before the replica<br/>   is completely initialised.<br>- Fixed an issue with Jiva Volumes where target can mark the Replica as Timed out on IO,<br/>   even when the Replica might actually be processing the Sync IO.<br>- Fixed an issue with Jiva Volumes that would not allow for Replicas to re-connect with the<br/>   Target, if the initial Registration failed to successfully process the hand-shake request.<br>- Fixed an issue with Jiva Volumes that would cause Target to restart when a send<br/>   diagnostic command was received from the client<br>- Fixed an issue causing PVC to be stuck in pending state, when there were more than<br/>   one PVCs associated with an Application Pod<br>- Toleration policy support for cStorStoragePool.<br/> |
| 0.8.1                            | [Release Blog](https://blog.openebs.io/openebs-releases-0-8-1-with-stability-fixes-and-improved-documentation-374dd6b7c4a5) <br> <br> [Release Notes](https://github.com/openebs/openebs/releases/tag/0.8.1) | Fixes an issue where cStor replica snapshots created for the rebuild were not deleted, causing space reclamation to fail.-Enhanced the metrics exported by cStor Pools to include details of the provisioning errors.-Enhanced the cStor Data Engine containers to contain troubleshooting utilities. <br>- Ephemeral Disk Support <br />- Enhanced the placement of cStor volume replica in a distributed randomly between the available pools.<br />- Enhanced the NDM to fetch additional details about the underlying disks via SeaChest.<br />- Enhanced the NDM  to add additional information to the DiskCRs like if the disks is partitioned or has a filesystem on it. <br />- Enhanced the OpenEBS CRDs to include custom columns to be displayed using  `kubectl get ` output of the CR. This feature requires K8s 1.11 or higher.<br />- Fixed an issue where cStor volume causes timeout for iSCSI discovery command and can potentially trigger a K8s vulnerability that can bring down a node with high RAM usage. |
| 0.8.0                            | [Release Blog](https://blog.openebs.io/openebs-0-8-release-allows-you-to-snapshot-and-clone-cstor-volumes-ebe09612f8b1) <br> <br> [Release Notes](https://github.com/openebs/openebs/releases/tag/0.8) | Fixes an issue where cStor volume used space was showing a very low value than actually used.-Fixes an issue where cStor replica snapshots created for the rebuild were not deleted, causing space reclamation to fail.-Enhanced the metrics exported by cStor Pools to include details of the provisioning errors.<br>- cStor Snapshot & Clone <br />-  cStor volume & Pool runtime status<br/>- Target Affinity for both Jiva & cStor <br/>- Target namespace for cStor <br/>- Enhance the volume metrics exporter<br/>- Enhance Jiva to clear up internal snapshot taken during   Replica rebuild<br/>- Enhance Jiva to support sync and unmap IOs<br/>- Enhance cStor for recreating pool by automatically selecting the disks. |
| 0.7.2                            | [Release Notes](https://github.com/openebs/openebs/releases/tag/0.7.2) | Fixes an issue where jiva replicas failed to register with its target if there was an error during initial registration.-Fixes an issue where cStor volume used space was showing a very low value than actually used.-Fixes an issue where cStor replica snapshots created for the rebuild were not deleted, causing space reclamation to fail.<br>- Support for   clearing space used by Jiva replica after the volume is deleted using Cron   Job.<br/>- Support for a storage policy that can disable the Jiva Volume Space   reclaim.<br/>- Support Target Affinity fort Jiva target Pod on the same node as the   Application Pod.<br/>- Enhanced Jiva related to internal snapshots for rebuilding Jiva.<br/> - Enhanced exporting cStor volume metrics to prometheus |
| 0.7.0                            | [Release Blog](https://blog.openebs.io/openebs-0-7-release-pushes-cstor-storage-engine-to-field-trials-1c41e6ad8c91)<br><br> [Release Notes](https://github.com/openebs/openebs/releases/tag/v0.7) | Fixes an issue where NDM would create a partitioned OS device as a block device.-Fixes an issue where jiva replicas failed to register with its target if there was an error during initial registration.-Fixes an issue where cStor volume used space was showing a very low value than actually used.<br>- Enhanced NDM   to discover block devices attached to Nodes     .<br/>- Alpha support for cStor Engine<br/>- Naming convention of Jiva Storage pool as 'default' and StorageClass as   'openebs-jiva-default'<br/>- Naming convention of cStor Storage pool as 'cstor-sparse-pool' and   StorageClass as 'openebs-cstor-sparse'<br/>- Support for specifying replica count,CPU/Memory Limits per PV,Choice of  Storage Engine, Nodes on which data copies should be copied. |
| 0.6.0                            | [Release Blog](https://blog.openebs.io/openebs-0-6-serves-ios-amidst-chaos-and-much-more-45c68eb59c6a)<br><br>[Release Notes](https://github.com/openebs/openebs/releases/tag/v0.6) | Fixes an issue where jiva replica data was not clean up if the PVC and its namespace were deleted prior to scrub job completion.-Fixes an issue where NDM would create a partitioned OS device as a block device.-Fixes an issue where jiva replicas failed to register with its target if there was an error during initial registration.<br>- Integrate  the Volume Snapshot capabilities with Kubernetes Snapshot controller.<br/>- Enhance maya-apiserver to use CAS Templates for orchestrating new   Storage Engines.<br/>- Enhance mayactl to show details about replica and Node details where replicas   are running.<br/>- Enhance maya-apiserver to schedule Replica Pods on specific nodes using   nodeSelector.<br/>- Enhance e2e tests to simulate chaos at different layers such as - CPU,   RAM, Disk, Network, and Node.<br/>- Support for deploying OpenEBS via Kubernetes stable Helm Charts.<br/>- Enhanced Jiva volume to handle more read only volume  scenarios |
| 0.5.4                            | [Release Notes](https://github.com/openebs/openebs/releases/tag/v0.5.4) | Fixes an issue where Velero Backup/Restore was not working with hostpath Local PVs.-Fixes an issue where jiva replica data was not clean up if the PVC and its namespace were deleted prior to scrub job completion.-Fixes an issue where NDM would create a partitioned OS device as a block device.<br> - Provision to   specify filesystems other than ext4 (default).<br/>- Support for XFS filesystem format for mongodb StatefulSet using OpenEBS Persistent Volume.<br/>- Increased integration test & e2e coverage in the CI<br/>- OpenEBS is now available as a stable chart from Kubernetes |
| 0.5.3                            | [Release Notes](https://github.com/openebs/openebs/releases/tag/v0.5.3) | Upgraded the base ubuntu images for the containers to fix the security vulnerabilities reported in Ubuntu Xenial.-Fixes an issue where Velero Backup/Restore was not working with hostpath Local PVs.-Fixes an issue where jiva replica data was not clean up if the PVC and its namespace were deleted prior to scrub job completion.<br>- Fixed usage of StoragePool issue when RBAC settings are applied<br/>- Enhanced memory consumption usage for Jiva Volume |
| 0.5.2                            | [Release Notes](https://github.com/openebs/openebs/releases/tag/v0.5.2) | Changed the custom resource (Disk) used in earlier releases has been changed to Block Device.-Upgraded the base ubuntu images for the containers to fix the security vulnerabilities reported in Ubuntu Xenial.-Fixes an issue where Velero Backup/Restore was not working with hostpath Local PVs.<br>- Support to   set non-SSL Kubernetes endpoints to use by specifying the ENV variables on  maya-apiserver and  openebs-provisioner. |
| 0.5.1                            | [Release Notes](https://github.com/openebs/openebs/releases/tag/v0.5.1) | -Changed the custom resource (Disk) used in earlier releases has been changed to Block Device.-Upgraded the base ubuntu images for the containers to fix the security vulnerabilities reported in Ubuntu Xenial.<br>- Support to   use Jiva volume from CentOS iSCSI Initiator<br/>- Support openebs-k8s-provisioner to be launched in non-default namespace |
| 0.5.0                            | [Release Blog](https://blog.openebs.io/openebs-0-5-0-enables-customizable-storage-engines-using-storage-policies-dc585d5ee2f) <br><br>[Release Notes](https://github.com/openebs/openebs/releases/tag/v0.5.0) | -Changed the custom resource (Disk) used in earlier releases has been changed to Block Device.<br>- Enhanced   Storage Policy Enforcement Framework for Jiva.<br/>- Extend OpenEBS API Server to expose volume snapshot API.<br/>- Support for deploying OpenEBS via helm charts.<br/>- Sample Prometheus configuration for collecting OpenEBS Volume Metrics.<br/>- Sample Grafana OpenEBS Volume Dashboard - using the prometheus Metrics |
| 0.4.0                            | [Release Blog](https://blog.openebs.io/quick-update-on-openebs-v0-4-a-developer-friendly-release-6fe599fe254e)<br><br>[Release Notes](https://github.com/openebs/openebs/releases/tag/v0.4.0) | - Enhanced   MAYA cli support for managing snapshots,usage statistics.<br/>- Support OpenEBS Maya API Server uses the Kubernetes scheduler logic to place OpenEBS Volume Replicas on different nodes<br/>- Support Extended deployment of OpenEBS in AWS.<br/>- Support OpenEBS can be deployed in a minikube setup.<br/>- Enhanced openebs-k8s-provisioner from crashloopbackoff state |
| 0.3.0                            | [Release Blog](https://blog.openebs.io/openebs-on-the-growth-path-releases-0-3-94bd45724e)<br><br>[Release Notes](https://github.com/openebs/openebs/releases/tag/v0.3) | - Support   OpenEBS hyper-converged with Kubernetes Minion Nodes.<br/>- Enable OpenEBS via the openebs-operator.yaml<br/>-  Supports creation of OpenEBS volumes using Dynamic Provisioner.<br/>- Storage functionality and Orchestration/Management functionality is delivered as container images on DockerHub. |
| 0.2.0                            | [Release Blog](https://blog.openebs.io/openebs-sprinting-ahead-0-2-released-28f5001deeaa)<br><br>[Release Notes](https://github.com/openebs/openebs/releases/tag/v0.2) | - Integrated   OpenEBS FlexVolume Driver and Dynamically Provision OpenEBS Volumes into Kubernetes.<br/>- Support Maya api server to provides new AWS EBS-like API for   provisioning Block Storage.<br/>- Enhanced Maya api server to Hyper Converged with Nomad Scheduler.<br/>- Backup/Restore Data from Amazon S3.<br/>- Node Failure Resiliency Fixes |

<br>





## See Also:

### [cStor Roadmap](/v130/docs/next/cstor.html#cstor-roadmap)

### [OpenEBS FAQ](/v130/docs/next/faq.html)

### [Container Attached Storage or CAS](/v130/docs/next/cas.html)

<br><hr>

<br>




<!-- Hotjar Tracking Code for https://docs.openebs.io -->
<script>
   (function(h,o,t,j,a,r){
       h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
       h._hjSettings={hjid:785693,hjsv:6};
       a=o.getElementsByTagName('head')[0];
       r=o.createElement('script');r.async=1;
       r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
       a.appendChild(r);
   })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-92076314-12"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-92076314-12');
</script>
